#!/usr/bin/env python3
"""GitHub Contributions Statistics Script

## æ ¸å¿ƒåŠŸèƒ½
- ç»Ÿè®¡æ‰€æœ‰ä»“åº“çš„è´¡çŒ®ï¼ˆadditions/deletionsã€imagesæ•°é‡ï¼‰
- æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼Œé¿å…é‡å¤åˆ†æå·²å¤„ç†çš„ commits
- æ”¯æŒ Fork ä»“åº“ï¼ˆç›´æ¥å…‹éš†ä¸Šæ¸¸ä»“åº“è·å–å®Œæ•´å†å²ï¼‰
- è‡ªåŠ¨æ›´æ–° README.md ç»Ÿè®¡æ•°æ®å’Œæ—¶é—´

## æ•°æ®æºç­–ç•¥
- Git log ä¼˜å…ˆï¼šå®Œæ•´å†å²æ•°æ®ï¼Œå‡†ç¡®å¯é 
- API ä»…å…œåº•ï¼šä»…åœ¨ git log å¤±è´¥æ—¶ä½¿ç”¨ï¼ˆæœ‰åˆ†é¡µé™åˆ¶ï¼Œæœ€å¤š 1000 æ¡ï¼‰

## Fork ä»“åº“å¤„ç†
- ç›´æ¥å…‹éš†ä¸Šæ¸¸ä»“åº“ï¼ˆä¸æ˜¯ Fork ä»“åº“æœ¬èº«ï¼‰
- ä» origin è·å– Git logï¼Œä¿è¯è·å–å®Œæ•´çš„ commit å†å²

## ç¼“å­˜æ¸…ç†ç­–ç•¥
- Git log æ¨¡å¼ï¼šå¯¹æ¯”æ‰€æœ‰ commitsï¼Œåˆ é™¤æ¶ˆå¤±çš„ï¼ˆè¢«å˜åŸº/å‹ç¼©/é‡å†™ï¼‰
- API å…œåº•æ¨¡å¼ï¼šåªå¯¹æ¯” API æ—¶é—´æˆ³èŒƒå›´å†…çš„ commitsï¼ŒèŒƒå›´å¤–çš„è€æ•°æ®ä¿ç•™

## ç¼“å­˜æœºåˆ¶
æ¯ä¸ªä»“åº“ä¸€ä¸ª JSON æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- å…ƒæ•°æ®ï¼šæ€» commits æ•°ã€æ€» additions/deletions æ•°ã€æ€» images æ•°
- è¯¦ç»†æ•°æ®ï¼šæ¯ä¸ª commit çš„ç»Ÿè®¡æ•°æ®

## æ›´æ–°ç­–ç•¥
- åªæœ‰è¿è¡Œè„šæœ¬æ—¶æ‰æ›´æ–°æ•°æ®
- ä½¿ç”¨æ­£åˆ™åŒ¹é…æ›¿æ¢ï¼Œä¿æŒ README.md åŸæœ‰æ ¼å¼
- æ°¸ä¹…ä¿å­˜å†å²æ•°æ®ï¼Œæ™ºèƒ½æ¸…ç†è¿‡æœŸç¼“å­˜
"""

import argparse
import base64
import json
import os
import re
import shutil
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import cast

# ============================================================================
# ç±»å‹å®šä¹‰
# ============================================================================

FileData = dict[str, str | int]
AuthorData = dict[str, str]
CommitDetailData = dict[str, AuthorData]
CommitData = dict[str, str | int | FileData | CommitDetailData | list[FileData]]
CacheData = dict[str, list[CommitData]]
RepoInfo = dict[str, str | bool | dict[str, str] | None]
StatsData = dict[str, int]


@dataclass
class RepoContext:
    """ä»“åº“ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºå‡å°‘å‡½æ•°å‚æ•°æ•°é‡"""

    repo_path: str  # æœ¬åœ°ä»“åº“è·¯å¾„
    owner: str  # ä»“åº“æ‰€æœ‰è€…ï¼ˆFork ä»“åº“æ—¶ä¸ºä¸Šæ¸¸ ownerï¼‰
    repo_name: str  # ä»“åº“åç§°ï¼ˆFork ä»“åº“æ—¶ä¸ºä¸Šæ¸¸åç§°ï¼‰
    username: str  # è¦ç»Ÿè®¡çš„ç”¨æˆ·å


# ============================================================================
# å¸¸é‡å®šä¹‰
# ============================================================================

GITHUB_API = "https://api.github.com"
SEPARATOR_LENGTH = 60
MAX_API_PAGES = 10
PER_PAGE = 100
PROGRESS_INTERVAL = 10
IMAGE_EXTENSIONS = [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp", ".ico"]
README_FILE_PATH = Path(__file__).parent.parent / "README.md"
CACHE_DIR = Path(__file__).parent / "stats_cache"
AUTHOR_IDENTITIES_FILE = CACHE_DIR / "author_identities.json"

# æ—¶é—´å’Œæ ¼å¼å¸¸é‡
TIME_FORMAT = "%Y-%m-%d %H:%M:%S UTC+8"
TIME_PATTERN = r"(Last updated: )\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}( UTC\+8)?"
STATS_PATTERN = r"(â•additions: )\d+( â–deletions: )\d+( ğŸ–¼ï¸images: )\d+"
STATS_EXTRACT_PATTERN = r"â•additions: (\d+) â–deletions: (\d+) ğŸ–¼ï¸images: (\d+)"

# Git è§£æå¸¸é‡
MIN_STATUS_PARTS = 2  # git show --name-status è¾“å‡ºè‡³å°‘éœ€è¦çš„å­—æ®µæ•°
MIN_NUMSTAT_PARTS = 3  # git show --numstat è¾“å‡ºè‡³å°‘éœ€è¦çš„å­—æ®µæ•°

# å ä½ç¬¦æ˜ å°„
PLACEHOLDER_MAPPINGS = {
    "ORIGIN_USERNAME": "ORIGIN_USERNAME",
    "UPSTREAM_USERNAME": "UPSTREAM_USERNAME",
    "TOTAL_ADDITIONS": "total_additions",
    "TOTAL_DELETIONS": "total_deletions",
    "TOTAL_IMAGES": "total_images",
    "LAST_UPDATED": "current_time",
}


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================


def get_default_from_readme(var_name: str) -> str | None:
    """ä» README.md ä¸­è¯»å–é»˜è®¤çš„ç”¨æˆ·åå˜é‡"""
    if not README_FILE_PATH.exists():
        return None

    try:
        with README_FILE_PATH.open(encoding="utf-8") as f:
            content = f.read()

        # æŸ¥æ‰¾å˜é‡å®šä¹‰
        pattern = rf"{var_name} = ([^\n\r]+)"
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
    except (OSError, UnicodeDecodeError):
        pass

    return None


# ============================================================================
# é…ç½®
# ============================================================================

ORIGIN_USERNAME = (
    os.environ.get("ORIGIN_USERNAME")
    or get_default_from_readme("ORIGIN_USERNAME")
    or ""
)
UPSTREAM_USERNAME = (
    os.environ.get("UPSTREAM_USERNAME")
    or get_default_from_readme("UPSTREAM_USERNAME")
    or ""
)
TOKEN = os.environ.get("GH_TOKEN")


# ============================================================================
# ä½œè€…èº«ä»½ç®¡ç†ï¼ˆè‡ªåŠ¨å­¦ä¹ ï¼‰
# ============================================================================

# è¿è¡Œæ—¶å·²çŸ¥çš„ä½œè€…èº«ä»½ï¼ˆè„šæœ¬å¯åŠ¨æ—¶ä»æ–‡ä»¶åŠ è½½ï¼‰
KNOWN_AUTHOR_IDENTITIES: set[str] = set()


def load_author_identities() -> set[str]:
    """åŠ è½½å·²çŸ¥çš„ä½œè€…èº«ä»½åˆ—è¡¨

    å­˜å‚¨æ ¼å¼: Base64 ç¼–ç çš„ JSON
    å…¼å®¹æ—§æ ¼å¼: çº¯ JSONï¼ˆè‡ªåŠ¨è¿ç§»åˆ°æ–°æ ¼å¼ï¼‰
    """
    if not AUTHOR_IDENTITIES_FILE.exists():
        return set()

    try:
        with AUTHOR_IDENTITIES_FILE.open(encoding="utf-8") as f:
            raw_data = f.read().strip()
            if not raw_data:
                return set()

            # å°è¯• Base64 è§£ç ï¼ˆæ–°æ ¼å¼ï¼‰
            try:
                decoded_bytes = base64.b64decode(raw_data)
                data = json.loads(decoded_bytes.decode("utf-8"))
            except (ValueError, UnicodeDecodeError):
                # å›é€€åˆ°çº¯ JSON è§£ç ï¼ˆæ—§æ ¼å¼å…¼å®¹ï¼‰
                data = json.loads(raw_data)
                # è§¦å‘è¿ç§»ï¼šä¸‹æ¬¡ä¿å­˜æ—¶ä¼šè‡ªåŠ¨è½¬æ¢ä¸º Base64 æ ¼å¼

            identities = set(data.get("identities", []))
            if identities:
                print_color(f"ğŸ’¾ å·²åŠ è½½ {len(identities)} ä¸ªå·²çŸ¥ä½œè€…èº«ä»½", Colors.GREEN)
            return identities
    except (OSError, json.JSONDecodeError) as e:
        print_color(f"âš ï¸  åŠ è½½ä½œè€…èº«ä»½å¤±è´¥: {e}", Colors.YELLOW)
        return set()


def save_author_identities(identities: set[str]) -> None:
    """ä¿å­˜ä½œè€…èº«ä»½åˆ—è¡¨ï¼ˆBase64 ç¼–ç ï¼Œäººç±»ä¸å¯è¯»ï¼‰"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    try:
        # æ„å»º JSON æ•°æ®
        data = {"identities": sorted(identities)}
        json_str = json.dumps(data, ensure_ascii=False)
        # Base64 ç¼–ç 
        encoded_data = base64.b64encode(json_str.encode("utf-8")).decode("ascii")

        with AUTHOR_IDENTITIES_FILE.open("w", encoding="utf-8") as f:
            f.write(encoded_data)
        print_color(f"ğŸ’¾ å·²ä¿å­˜ {len(identities)} ä¸ªä½œè€…èº«ä»½", Colors.GREEN)
    except OSError as e:
        print_color(f"âš ï¸  ä¿å­˜ä½œè€…èº«ä»½å¤±è´¥: {e}", Colors.YELLOW)


def extract_author_from_commit(commit: CommitData) -> str | None:
    """ä» commit å¯¹è±¡æå– 'Name <email>' æ ¼å¼çš„ä½œè€…èº«ä»½"""
    commit_dict = commit.get("commit", {})
    if not isinstance(commit_dict, dict):
        return None

    author_dict = commit_dict.get("author", {})
    if not isinstance(author_dict, dict):
        return None

    name = author_dict.get("name", "")
    email = author_dict.get("email", "")
    if name and email:
        return f"{name} <{email}>"
    return None


def learn_author_identities_from_api(
    owner: str,
    repo_name: str,
    username: str,
) -> set[str]:
    """ä» GitHub API å­¦ä¹ ç”¨æˆ·çš„ä½œè€…èº«ä»½

    é€šè¿‡ API è·å–ç”¨æˆ·çš„ commitsï¼Œæå–æ‰€æœ‰ä¸åŒçš„ author èº«ä»½
    """
    print_color("    ğŸ” ä» API å­¦ä¹ ä½œè€…èº«ä»½...", Colors.YELLOW)

    identities: set[str] = set()
    page = 1

    while page <= 3:  # åªæŸ¥å‰ 3 é¡µï¼Œè¶³å¤Ÿå­¦ä¹ èº«ä»½
        api_url = (
            f"{GITHUB_API}/repos/{owner}/{repo_name}/commits"
            f"?author={username}&per_page={PER_PAGE}&page={page}"
        )
        output, returncode = github_api_request(api_url)

        if returncode != 0:
            break

        try:
            commits: list[CommitData] = json.loads(output)
            if not commits:
                break

            for commit in commits:
                if identity := extract_author_from_commit(commit):
                    identities.add(identity)

            if len(commits) < PER_PAGE:
                break
            page += 1

        except json.JSONDecodeError:
            break

    if identities:
        print_color(f"    âœ… å‘ç° {len(identities)} ä¸ªä½œè€…èº«ä»½", Colors.GREEN)
        for identity in sorted(identities):
            print_color(f"       - {identity}", Colors.NC)

    return identities


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================


class Colors:
    """ç»ˆç«¯é¢œè‰²å®šä¹‰"""

    RED = "\033[0;31m"
    GREEN = "\033[0;32m"
    YELLOW = "\033[1;33m"
    NC = "\033[0m"


def print_color(message: str, color: str = Colors.NC) -> None:
    """å½©è‰²è¾“å‡º"""
    print(f"{color}{message}{Colors.NC}")


def print_separator(title: str | None = None, color: str = Colors.GREEN) -> None:
    """æ‰“å°åˆ†éš”çº¿ï¼Œå¯é€‰æ ‡é¢˜"""
    separator = "=" * SEPARATOR_LENGTH
    print_color(separator, color)
    if title:
        print_color(title, color)
        print_color(separator, color)


def handle_error(
    operation: str,
    error: Exception,
    return_value: str | None = None,
) -> str | None:
    """ç»Ÿä¸€çš„é”™è¯¯å¤„ç†"""
    print_color(f"âŒ {operation}å¤±è´¥: {error}", Colors.RED)
    return return_value


def is_image_file(filename: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºå›¾ç‰‡"""
    return any(filename.lower().endswith(ext) for ext in IMAGE_EXTENSIONS)


def print_stats_summary(
    additions: int,
    deletions: int,
    images: int,
    *,
    include_images: bool = True,
    prefix: str = "",
) -> None:
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    print_color(
        f"{prefix}âœ… ä»£ç è´¡çŒ®: +{additions} additions, -{deletions} deletions",
        Colors.GREEN,
    )
    if include_images:
        print_color(f"{prefix}âœ… å›¾ç‰‡è´¡çŒ®: {images} images", Colors.GREEN)


def run_command(cmd: str, cwd: str | None = None) -> tuple[str, int]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›è¾“å‡º"""
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            cwd=cwd,
            check=False,
        )
        return result.stdout.strip(), result.returncode
    except (OSError, subprocess.SubprocessError) as e:
        print_color(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}", Colors.RED)
        return "", 1


def github_api_request(api_url: str) -> tuple[str, int]:
    """æ‰§è¡Œ GitHub API è¯·æ±‚

    ç»Ÿä¸€å°è£… curl å‘½ä»¤ï¼ŒåŒ…å«è®¤è¯å¤´å’Œ Accept å¤´ã€‚

    è¿”å›: (output, returncode)
    """
    curl_cmd = (
        f'curl -s -H "Authorization: token {TOKEN}" '
        f'-H "Accept: application/vnd.github.v3+json" "{api_url}"'
    )
    return run_command(curl_cmd)


def replace_placeholders(content: str, replacements: dict[str, str]) -> str:
    """é€šç”¨å ä½ç¬¦æ›¿æ¢å‡½æ•°"""
    for placeholder, value in replacements.items():
        content = content.replace(f"{{{{{placeholder}}}}}", str(value))
    return content


def update_variable_definition(content: str, var_name: str, var_value: str) -> str:
    """é€šç”¨å˜é‡å®šä¹‰æ›´æ–°å‡½æ•°"""
    pattern = rf"({var_name} = )([^\n\r]+)"
    if re.search(pattern, content):
        content = re.sub(pattern, f"\\1{var_value}", content)
        print_color(f"âœ… å·²æ›´æ–° {var_name} å®šä¹‰ä¸º: {var_value}", Colors.GREEN)
    return content


def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
    china_tz = timezone(timedelta(hours=8))
    return datetime.now(china_tz).strftime(TIME_FORMAT)


def calculate_cache_statistics(cache_data: CacheData) -> tuple[int, int, int, int]:
    """è®¡ç®—ç¼“å­˜æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯

    è¿”å›: (total_commits, total_additions, total_deletions, total_images)
    """
    total_commits = 0
    total_additions = 0
    total_deletions = 0
    total_images = 0

    for commits in cache_data.values():
        commits_list: list[CommitData] = commits
        # æ–°æ ¼å¼ï¼šæ•°ç»„ç»“æ„
        total_commits += len(commits_list)
        for commit in commits_list:
            commit_dict = cast("dict[str, str | int]", commit)
            additions = commit_dict.get("additions", 0)
            deletions = commit_dict.get("deletions", 0)
            images = commit_dict.get("images", 0)
            total_additions += additions if isinstance(additions, int) else 0
            total_deletions += deletions if isinstance(deletions, int) else 0
            total_images += images if isinstance(images, int) else 0

    return total_commits, total_additions, total_deletions, total_images


def sort_and_reindex_commits(cache_data: CacheData) -> CacheData:
    """å¯¹ç¼“å­˜æ•°æ®è¿›è¡Œæ’åºå’Œé‡æ–°ç¼–å·"""
    sorted_cache_data: CacheData = {}

    for repo_name, commits in cache_data.items():
        # æŒ‰ timestamp ä»æ—§åˆ°æ–°æ’åºï¼ˆè€çš„åœ¨å‰ï¼Œæ–°çš„åœ¨åï¼‰
        sorted_commits: list[CommitData] = sorted(
            commits,
            key=lambda x: str(x.get("timestamp", "")),
        )

        # é‡æ–°ç¼–å· indexï¼ˆä» 1 å¼€å§‹ï¼‰
        for idx, commit in enumerate(sorted_commits, start=1):
            commit["index"] = idx

        sorted_cache_data[repo_name] = sorted_commits

    return sorted_cache_data


# ============================================================================
# ç¼“å­˜ç®¡ç†
# ============================================================================


def load_cache(repo_name: str) -> CacheData:
    """åŠ è½½æŒ‡å®šä»“åº“çš„ç¼“å­˜æ•°æ®"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_file = CACHE_DIR / f"{repo_name}.json"

    try:
        with cache_file.open(encoding="utf-8") as f:
            cache_data = json.load(f)
            print_color(f"ğŸ’¾ å·²åŠ è½½ç¼“å­˜: {cache_file}", Colors.GREEN)

            metadata = cache_data.get("_metadata", {})
            print_color(
                f"   ç¼“å­˜åŒ…å« {metadata.get('total_commits', 0)} ä¸ªcommits",
                Colors.NC,
            )
            return cache_data.get("data", {})
    except (OSError, json.JSONDecodeError) as e:
        print_color(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}", Colors.YELLOW)
        return {}


def save_cache(repo_name: str, cache_data: CacheData) -> bool:
    """ä¿å­˜æŒ‡å®šä»“åº“çš„ç¼“å­˜æ•°æ®

    åŠŸèƒ½ï¼š
    - æŒ‰æ—¶é—´æˆ³æ’åº commitsï¼ˆä»æ—§åˆ°æ–°ï¼‰
    - é‡æ–°ç¼–å· commit indexï¼ˆä» 1 å¼€å§‹ï¼‰
    - ç»Ÿè®¡æ€» commits æ•°ã€æ€»å¢åˆ è¡Œæ•°å’Œæ€»å›¾ç‰‡æ•°
    - ä¿å­˜ä¸ºå¸¦ metadata çš„ JSON æ ¼å¼

    å‚æ•°ï¼š
    - repo_name: ä»“åº“åç§°
    - cache_data: ç¼“å­˜æ•°æ®å­—å…¸

    JSON è¾“å‡ºæ ¼å¼ï¼š
    {
      "_metadata": {
        "total_commits": int,    // æ€» commit æ•°
        "total_additions": int,  // æ€»å¢åŠ è¡Œæ•°
        "total_deletions": int,  // æ€»åˆ é™¤è¡Œæ•°
        "total_images": int      // æ€»å›¾ç‰‡æ•°
      },
      "data": { ... }            // commit æ•°æ®
    }
    """
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_file = CACHE_DIR / f"{repo_name}.json"

    try:
        # æ’åºå’Œé‡æ–°ç¼–å·
        sorted_cache_data = sort_and_reindex_commits(cache_data)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_commits, total_additions, total_deletions, total_images = (
            calculate_cache_statistics(sorted_cache_data)
        )

        cache_data_with_metadata: dict[str, dict[str, int] | CacheData] = {
            "_metadata": {
                "total_commits": total_commits,
                "total_additions": total_additions,
                "total_deletions": total_deletions,
                "total_images": total_images,
            },
            "data": sorted_cache_data,
        }

        with cache_file.open("w", encoding="utf-8") as f:
            json.dump(cache_data_with_metadata, f, indent=2, ensure_ascii=False)

        print_color(f"âœ… ç¼“å­˜å·²ä¿å­˜: {cache_file}", Colors.GREEN)
        print_color(f"   commits: {total_commits}", Colors.NC)
        print_color(f"   additions: {total_additions}", Colors.NC)
        print_color(f"   deletions: {total_deletions}", Colors.NC)
        print_color(f"   images: {total_images}", Colors.NC)
    except (OSError, TypeError, ValueError) as e:
        print_color(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}", Colors.RED)
        return False
    else:
        return True


def extract_sha_from_cache_item(item: str | CommitData, *, is_list_format: bool) -> str:
    """ä»ç¼“å­˜é¡¹ä¸­æå–SHAå€¼"""
    if is_list_format:
        if isinstance(item, dict):
            url = item.get("url", "")
            if isinstance(url, str):
                return url.split("/")[-1] if url else ""
        return ""
    return item if isinstance(item, str) else ""


def _extract_commit_timestamps(
    commits: list[CommitData],
) -> tuple[set[str], str, str]:
    """ä» commits æå– SHA é›†åˆå’Œæ—¶é—´æˆ³èŒƒå›´

    è¿”å›: (sha_set, min_timestamp, max_timestamp)
    """
    sha_set: set[str] = set()
    min_ts = ""
    max_ts = ""

    for commit in commits:
        sha = commit.get("sha")
        if isinstance(sha, str):
            sha_set.add(sha)

        commit_dict = commit.get("commit", {})
        if isinstance(commit_dict, dict):
            author_dict = commit_dict.get("author", {})
            if isinstance(author_dict, dict):
                ts = author_dict.get("date", "")
                if ts:
                    if not min_ts or ts < min_ts:
                        min_ts = ts
                    if not max_ts or ts > max_ts:
                        max_ts = ts

    return sha_set, min_ts, max_ts


def _partition_cached_items(
    cached_items: list[CommitData],
    min_timestamp: str,
    max_timestamp: str,
    *,
    is_api_fallback: bool,
) -> tuple[list[CommitData], list[CommitData]]:
    """å°†ç¼“å­˜é¡¹åˆ†ä¸ºèŒƒå›´å†…å’ŒèŒƒå›´å¤–ä¸¤ç»„

    è¿”å›: (in_range_items, out_of_range_items)
    """
    in_range: list[CommitData] = []
    out_of_range: list[CommitData] = []

    for item in cached_items:
        item_ts = item.get("timestamp", "")
        if isinstance(item_ts, str):
            if (
                is_api_fallback
                and min_timestamp
                and max_timestamp
                and item_ts < min_timestamp
            ):
                out_of_range.append(item)
            else:
                in_range.append(item)

    return in_range, out_of_range


def _log_cache_cleanup_mode(
    *,
    is_api_fallback: bool,
    min_timestamp: str,
    max_timestamp: str,
    out_of_range_count: int,
) -> None:
    """æ‰“å°ç¼“å­˜æ¸…ç†æ¨¡å¼ä¿¡æ¯"""
    if is_api_fallback:
        min_ts = min_timestamp[:10] if min_timestamp else "?"
        max_ts = max_timestamp[:10] if max_timestamp else "?"
        mode_desc = f"API å…œåº•æ¨¡å¼ï¼ˆæ£€æŸ¥èŒƒå›´: {min_ts} ~ {max_ts}ï¼‰"
        if out_of_range_count > 0:
            print_color(f"    â„¹ï¸  {mode_desc}", Colors.NC)
            print_color(
                f"       ä¿ç•™ {out_of_range_count} ä¸ªè¶…å‡º API èŒƒå›´çš„è€æ•°æ®", Colors.NC
            )
    else:
        print_color("    â„¹ï¸  Git log æ¨¡å¼ï¼ˆå®Œæ•´å†å²ï¼‰", Colors.NC)


def clean_stale_cache(
    cache_data: CacheData,
    current_commits_with_data: list[CommitData],
    repo_key: str,
    *,
    is_api_fallback: bool = False,
) -> CacheData:
    """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜ï¼ˆæ£€æµ‹å˜åŸºç­‰å¯¼è‡´çš„commitå“ˆå¸Œå˜åŒ–ï¼‰

    ç­–ç•¥ï¼š
    - Git log æ¨¡å¼ï¼ˆå®Œæ•´å†å²ï¼‰ï¼šå¯¹æ¯”æ‰€æœ‰ commitsï¼Œåˆ é™¤æ¶ˆå¤±çš„
    - API å…œåº•æ¨¡å¼ï¼šåªå¯¹æ¯” API è¿”å›çš„æ—¶é—´æˆ³èŒƒå›´å†…çš„ commitsï¼ŒèŒƒå›´å¤–çš„ä¿ç•™

    å‚æ•°ï¼š
    - current_commits_with_data: å½“å‰æ•°æ®æºçš„ commit å¯¹è±¡åˆ—è¡¨ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    - is_api_fallback: æ˜¯å¦ä¸º API å…œåº•æ¨¡å¼
    """
    if repo_key not in cache_data:
        return cache_data

    # æå–å½“å‰ commits çš„ SHA é›†åˆå’Œæ—¶é—´æˆ³èŒƒå›´
    current_commit_set, min_timestamp, max_timestamp = _extract_commit_timestamps(
        current_commits_with_data,
    )

    # å°†ç¼“å­˜é¡¹åˆ†ç»„
    cached_items_in_range, cached_items_out_of_range = _partition_cached_items(
        cache_data[repo_key],
        min_timestamp,
        max_timestamp,
        is_api_fallback=is_api_fallback,
    )

    # è·å–èŒƒå›´å†…ç¼“å­˜çš„ SHA é›†åˆ
    cached_shas_in_range: set[str] = {
        sha
        for item in cached_items_in_range
        if (sha := extract_sha_from_cache_item(item, is_list_format=True))
    }

    # æ‰¾å‡ºæ¶ˆå¤±çš„ commits
    stale_commits = cached_shas_in_range - current_commit_set

    # æ‰“å°æ¨¡å¼ä¿¡æ¯
    _log_cache_cleanup_mode(
        is_api_fallback=is_api_fallback,
        min_timestamp=min_timestamp,
        max_timestamp=max_timestamp,
        out_of_range_count=len(cached_items_out_of_range),
    )

    # å¤„ç†è¿‡æœŸç¼“å­˜
    if stale_commits:
        print_color(
            f"    ğŸ§¹ æ£€æµ‹åˆ° {len(stale_commits)} ä¸ªæ¶ˆå¤±çš„commits", Colors.YELLOW
        )
        print_color("       åŸå› ï¼šè¢«å˜åŸºã€å‹ç¼©æˆ–é‡å†™", Colors.YELLOW)

        # ä¿ç•™èŒƒå›´å¤–çš„ + èŒƒå›´å†…æœªè¿‡æœŸçš„
        new_cache_list = list(cached_items_out_of_range) + [
            item
            for item in cached_items_in_range
            if extract_sha_from_cache_item(item, is_list_format=True)
            not in stale_commits
        ]
        cache_data[repo_key] = new_cache_list

        print_color(
            f"    âœ… å·²æ¸…é™¤ {len(stale_commits)} ä¸ªè¿‡æœŸçš„commitç¼“å­˜", Colors.GREEN
        )

        if not cache_data[repo_key]:
            del cache_data[repo_key]
            print_color("    â„¹ï¸  ä»“åº“ç¼“å­˜å·²æ¸…ç©º", Colors.NC)
    else:
        print_color("    âœ… ç¼“å­˜æ•°æ®å®Œæ•´ï¼Œæ— æ¶ˆå¤±çš„commits", Colors.GREEN)

    return cache_data


# ============================================================================
# GitHub API æ“ä½œ
# ============================================================================


def get_repos() -> list[RepoInfo]:
    """è·å–ç”¨æˆ·çš„æ‰€æœ‰ä»“åº“ï¼ˆæ”¯æŒåˆ†é¡µï¼Œç§æœ‰ä»“åº“éœ€è¦ PAT æƒé™ï¼‰"""
    print_color("ğŸ“¡ è·å–æ‰€æœ‰ä»“åº“åˆ—è¡¨...", Colors.YELLOW)

    repos: list[RepoInfo] = []
    page = 1
    max_pages = MAX_API_PAGES

    while page <= max_pages:
        api_url = (
            f"{GITHUB_API}/users/{ORIGIN_USERNAME}/repos"
            f"?per_page={PER_PAGE}&type=all&page={page}"
        )
        output, returncode = github_api_request(api_url)

        if returncode != 0:
            print_color("âŒ è·å–ä»“åº“åˆ—è¡¨å¤±è´¥", Colors.RED)
            return repos if repos else []

        try:
            parsed_data: list[RepoInfo] | dict[str, str] = json.loads(output)

            # æ£€æŸ¥ API é”™è¯¯å“åº”
            if isinstance(parsed_data, dict):
                error_msg = parsed_data.get("message", "")
                if error_msg:
                    print_color(f"âŒ API é”™è¯¯: {error_msg}", Colors.RED)
                    return repos if repos else []
                break  # dict ä½†æ—  messageï¼Œå¼‚å¸¸æƒ…å†µ

            if not parsed_data:
                break

            for repo in parsed_data:
                repo_info: RepoInfo = repo
                repos.append(repo_info)

            print_color(f"   ç¬¬ {page} é¡µï¼šè·å–åˆ° {len(parsed_data)} ä¸ªä»“åº“", Colors.NC)

            if len(parsed_data) < PER_PAGE:
                break

            page += 1

        except json.JSONDecodeError as e:
            print_color(f"âŒ JSON è§£æå¤±è´¥: {e}", Colors.RED)
            print_color(f"æ•°æ®å†…å®¹: {output[:500]}", Colors.RED)
            return repos if repos else []

    print_color(f"âœ… è·å–åˆ° {len(repos)} ä¸ªä»“åº“", Colors.GREEN)
    for repo in repos:
        repo_name = repo.get("name", "Unknown")
        is_fork = repo.get("fork", False)
        print_color(
            f"   - {repo_name} ({'Fork' if is_fork else 'åŸåˆ›'})",
            Colors.NC,
        )
    return repos


def get_upstream_repo(repo: RepoInfo) -> tuple[str | None, str | None]:
    """è·å– fork ä»“åº“çš„ä¸Šæ¸¸ä»“åº“ä¿¡æ¯

    æ³¨æ„ï¼š/users/{username}/repos åˆ—è¡¨ API ä¸è¿”å› source/parent å­—æ®µï¼Œ
    éœ€è¦é¢å¤–è°ƒç”¨ /repos/{owner}/{repo} è·å–è¯¦æƒ…ã€‚
    """
    is_fork = repo.get("fork")
    if not isinstance(is_fork, bool) or not is_fork:
        return None, None

    # åˆ—è¡¨ API å¯èƒ½å·²åŒ…å« source/parentï¼ˆæŸäº›æƒ…å†µä¸‹ï¼‰
    upstream_info = repo.get("source") or repo.get("parent")

    # å¦‚æœåˆ—è¡¨ API æœªè¿”å›ä¸Šæ¸¸ä¿¡æ¯ï¼Œé¢å¤–è°ƒç”¨è¯¦æƒ… API
    if not upstream_info:
        repo_name = repo.get("name")
        if not repo_name:
            return None, None

        api_url = f"{GITHUB_API}/repos/{ORIGIN_USERNAME}/{repo_name}"
        output, returncode = github_api_request(api_url)
        if returncode == 0:
            try:
                repo_detail = json.loads(output)
                upstream_info = repo_detail.get("source") or repo_detail.get("parent")
            except json.JSONDecodeError:
                return None, None

    if isinstance(upstream_info, dict):
        upstream_dict: dict[str, str | dict[str, str]] = cast(
            "dict[str, str | dict[str, str]]",
            upstream_info,
        )
        owner_dict = upstream_dict.get("owner")
        if isinstance(owner_dict, dict):
            upstream_owner = owner_dict.get("login")
            upstream_name = upstream_dict.get("name")
            if isinstance(upstream_owner, str) and isinstance(upstream_name, str):
                print_color(
                    f"    ğŸ“¡ æ£€æµ‹åˆ°ä¸Šæ¸¸ä»“åº“: {upstream_owner}/{upstream_name}",
                    Colors.NC,
                )
                return upstream_owner, upstream_name
    return None, None


# ============================================================================
# Commits æ•°æ®è·å–
# ============================================================================


def get_commits_from_git_log(
    repo_path: str,
    default_branch: str,
) -> list[CommitData] | None:
    """ä»æœ¬åœ° git log è·å–ç”¨æˆ·çš„æ‰€æœ‰ commitsï¼ˆå®Œæ•´å†å²ï¼‰

    ä½œè€…åŒ¹é…ï¼šä»…ä½¿ç”¨ API å­¦ä¹ åˆ°çš„å®Œæ•´èº«ä»½ "Name <email>"
    è¿™ç¡®ä¿åªåŒ¹é…å±äºç”¨æˆ·çš„ commitsï¼Œé˜²æ­¢åŒåå†’å……

    è¿”å›åŒ…å« sha å’Œ commit.author.date çš„å®Œæ•´ç»“æ„ï¼Œæ–¹ä¾¿æ—¶é—´æˆ³æ¯”è¾ƒ
    """
    # ä»…ä½¿ç”¨å·²çŸ¥èº«ä»½ï¼ˆä» API è‡ªåŠ¨å­¦ä¹ çš„ "Name <email>" æ ¼å¼ï¼‰
    # ä¸ä½¿ç”¨å•ç‹¬çš„ç”¨æˆ·åï¼Œé˜²æ­¢åŒåå†’å……
    if not KNOWN_AUTHOR_IDENTITIES:
        print_color("    âš ï¸  æ²¡æœ‰å·²çŸ¥ä½œè€…èº«ä»½ï¼Œéœ€è¦å…ˆä» API å­¦ä¹ ", Colors.YELLOW)
        return None

    authors = KNOWN_AUTHOR_IDENTITIES
    print_color(f"    â„¹ï¸  ä½¿ç”¨ {len(authors)} ä¸ªä½œè€…èº«ä»½åŒ¹é…", Colors.NC)

    # ä½¿ç”¨é›†åˆå»é‡ï¼ˆé¿å…åŒä¸€ commit è¢«å¤šæ¬¡åŒ¹é…ï¼‰
    all_shas: set[str] = set()
    all_commits: list[CommitData] = []

    for author in authors:
        git_cmd = (
            f'git log origin/{default_branch} --author="{author}" --format="%H%n%aI"'
        )
        output, returncode = run_command(git_cmd, cwd=repo_path)

        if returncode != 0:
            continue

        lines = [line.strip() for line in output.split("\n") if line.strip()]

        # æ¯ä¸¤è¡Œä¸ºä¸€å¯¹ï¼šSHA å’Œ ISO æ—¶é—´æˆ³
        for i in range(0, len(lines), 2):
            if i + 1 < len(lines):
                sha = lines[i]
                iso_date = lines[i + 1]

                # è·³è¿‡å·²æ·»åŠ çš„ commitï¼ˆå»é‡ï¼‰
                if sha in all_shas:
                    continue
                all_shas.add(sha)

                # æ„å»ºä¸ API å…¼å®¹çš„ç»“æ„
                commit_obj: CommitData = {
                    "sha": sha,
                    "commit": {"author": {"date": iso_date}},
                }
                all_commits.append(commit_obj)

    if all_commits:
        print_color(
            f"    â„¹ï¸  git log è·å– {len(all_commits)} ä¸ªcommits",
            Colors.NC,
        )
        return all_commits
    print_color("    âš ï¸  git log å¤±è´¥", Colors.YELLOW)
    return None


def get_commits_from_api(
    owner: str,
    repo_name: str,
    username: str,
    default_branch: str = "main",
    max_pages: int = MAX_API_PAGES,
) -> list[CommitData]:
    """ä» GitHub API è·å–ç”¨æˆ·çš„æœ€è¿‘ commitsï¼ˆåˆ†é¡µï¼Œæœ€å¤š 10 é¡µï¼‰

    ä»…ä½œä¸º git log å¤±è´¥æ—¶çš„å…œåº•æ–¹æ¡ˆ
    æ³¨æ„: API æœ‰åˆ†é¡µé™åˆ¶ï¼Œè¶…å‡ºèŒƒå›´çš„è€ commits å°†ä¿ç•™ç¼“å­˜
    """
    page = 1
    per_page = PER_PAGE
    all_commits: list[CommitData] = []

    while page <= max_pages:
        api_url = (
            f"{GITHUB_API}/repos/{owner}/{repo_name}/commits"
            f"?author={username}&sha={default_branch}"
            f"&per_page={per_page}&page={page}"
        )
        print_color(f"    ğŸ” API è·å–commits (ç¬¬{page}é¡µ)...", Colors.NC)

        output, returncode = github_api_request(api_url)

        if returncode != 0:
            print_color("    âŒ APIè°ƒç”¨å¤±è´¥", Colors.RED)
            return all_commits if all_commits else []

        try:
            parsed_commits: list[
                dict[str, str | int | FileData | CommitDetailData | list[FileData]]
            ] = json.loads(output)
            if not parsed_commits:
                break

            for commit in parsed_commits:
                commit_data: CommitData = commit
                all_commits.append(commit_data)

            print_color(f"    ğŸ“Š å·²è·å– {len(all_commits)} ä¸ªcommits", Colors.NC)

            if len(parsed_commits) < per_page:
                break

            page += 1

        except json.JSONDecodeError as e:
            print_color(f"    âŒ JSON è§£æå¤±è´¥: {e}", Colors.RED)
            return all_commits if all_commits else []

    if page > max_pages:
        print_color(
            f"    â„¹ï¸  å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({max_pages} é¡µ)ï¼Œ"
            f"å…± {len(all_commits)} ä¸ªcommits",
            Colors.NC,
        )

    return all_commits


# ============================================================================
# Commits åˆ†æ
# ============================================================================


def _fetch_commits_with_fallback(
    ctx: RepoContext,
    default_branch: str,
) -> tuple[list[CommitData], bool]:
    """è·å– commitsï¼ŒGit log ä¼˜å…ˆï¼ŒAPI å…œåº•

    å› ä¸º Fork ä»“åº“ç›´æ¥å…‹éš†çš„æ˜¯ä¸Šæ¸¸ä»“åº“ï¼Œæ‰€ä»¥ç»Ÿä¸€ä» origin è·å–å³å¯ã€‚

    è¿”å›: (commits, is_api_fallback)
    """
    # æ¯æ¬¡éƒ½ä» API å¢é‡å­¦ä¹ èº«ä»½ï¼ˆæ”¯æŒç”¨æˆ·æ”¹ååœºæ™¯ï¼‰
    # API åªæŸ¥å‰å‡ é¡µï¼Œæ€§èƒ½å½±å“å¾ˆå°
    new_identities = learn_author_identities_from_api(
        ctx.owner, ctx.repo_name, ctx.username
    )
    if new_identities:
        old_count = len(KNOWN_AUTHOR_IDENTITIES)
        KNOWN_AUTHOR_IDENTITIES.update(new_identities)
        if len(KNOWN_AUTHOR_IDENTITIES) > old_count:
            print_color(
                f"    âœ… å‘ç°æ–°èº«ä»½ï¼Œå…± {len(KNOWN_AUTHOR_IDENTITIES)} ä¸ª",
                Colors.GREEN,
            )
            save_author_identities(KNOWN_AUTHOR_IDENTITIES)

    # 1. ä¼˜å…ˆå°è¯•ä»æœ¬åœ° git log è·å–ï¼ˆä½¿ç”¨å·²å­¦ä¹ çš„èº«ä»½ï¼‰
    if ctx.repo_path:
        commits = get_commits_from_git_log(ctx.repo_path, default_branch)
        if commits:
            print_color(
                f"    âœ… ä½¿ç”¨ Git logï¼ˆå®Œæ•´å†å²ï¼‰: {len(commits)} ä¸ªcommits",
                Colors.GREEN,
            )
            return commits, False

    # 2. Git log å¤±è´¥æ—¶ï¼Œä½¿ç”¨ API å…œåº•
    print_color("    âš ï¸  Git log æ— æ•°æ®ï¼Œå°è¯• API å…œåº•...", Colors.YELLOW)

    api_commits = get_commits_from_api(
        ctx.owner, ctx.repo_name, ctx.username, default_branch
    )

    if api_commits:
        print_color(
            f"    âœ… ä½¿ç”¨ API å…œåº•æ•°æ®: {len(api_commits)} ä¸ªcommits", Colors.GREEN
        )
        print_color(
            "    âš ï¸  æ³¨æ„: API æœ‰åˆ†é¡µé™åˆ¶ï¼Œè¶…å‡ºèŒƒå›´çš„è€æ•°æ®å°†ä¿ç•™ç¼“å­˜", Colors.YELLOW
        )
        return api_commits, True

    return [], False


def _find_cached_commit(
    cache_data: CacheData,
    repo_name: str,
    commit_url: str,
) -> CommitData | None:
    """åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ commit"""
    if repo_name not in cache_data:
        return None
    for item in cache_data[repo_name]:
        if item.get("url") == commit_url:
            return item
    return None


def _get_commit_details_from_git(
    repo_path: str,
    sha: str,
) -> CommitData:
    """ä»æœ¬åœ° git è·å– commit è¯¦æƒ…"""
    commit_data: CommitData = {}

    # è·å–æ–‡ä»¶çŠ¶æ€
    git_cmd = f'git show --name-status --pretty="" {sha}'
    status_output, _ = run_command(git_cmd, cwd=repo_path)

    # è·å–è¡Œæ•°ç»Ÿè®¡
    git_cmd = f'git show --numstat --pretty="" {sha}'
    numstat_output, returncode = run_command(git_cmd, cwd=repo_path)

    if returncode != 0:
        return commit_data

    commit_data["files"] = []

    # æ„å»ºçŠ¶æ€æ˜ å°„
    status_map: dict[str, str] = {}
    for line in status_output.split("\n"):
        if line.strip():
            parts = line.split("\t")
            if len(parts) >= MIN_STATUS_PARTS:
                status_map[parts[1]] = parts[0]

    # å¤„ç†è¡Œæ•°ç»Ÿè®¡
    for line in numstat_output.split("\n"):
        if line.strip():
            parts = line.split("\t")
            if len(parts) >= MIN_NUMSTAT_PARTS:
                try:
                    add_count = int(parts[0]) if parts[0] != "-" else 0
                    del_count = int(parts[1]) if parts[1] != "-" else 0
                    filename = parts[2]
                    file_status = status_map.get(filename, "modified")
                    commit_data["files"].append(
                        {
                            "additions": add_count,
                            "deletions": del_count,
                            "filename": filename,
                            "status": "added" if file_status == "A" else "modified",
                        }
                    )
                except ValueError:
                    continue

    return commit_data


def _get_commit_details_from_api(owner: str, repo_name: str, sha: str) -> CommitData:
    """ä» API è·å– commit è¯¦æƒ…"""
    api_url = f"{GITHUB_API}/repos/{owner}/{repo_name}/commits/{sha}"
    output, returncode = github_api_request(api_url)
    if returncode == 0:
        try:
            return json.loads(output)
        except json.JSONDecodeError:
            pass
    return {}


def _calculate_commit_stats(
    commit_data: CommitData,
    *,
    include_images: bool,
) -> tuple[int, int, int]:
    """è®¡ç®—å•ä¸ª commit çš„ç»Ÿè®¡æ•°æ®

    è¿”å›: (additions, deletions, images)
    """
    additions = 0
    deletions = 0
    images = 0

    files_list = commit_data.get("files", [])
    if isinstance(files_list, list):
        for file in files_list:
            file_additions = file.get("additions", 0)
            file_deletions = file.get("deletions", 0)
            if isinstance(file_additions, int):
                additions += file_additions
            if isinstance(file_deletions, int):
                deletions += file_deletions

            if include_images and file.get("status") == "added":
                filename = file.get("filename", "")
                if isinstance(filename, str) and is_image_file(filename):
                    images += 1

    return additions, deletions, images


def _get_commit_timestamp(commit: CommitData) -> str:
    """ä» commit è·å–æ—¶é—´æˆ³"""
    commit_dict = commit.get("commit", {})
    if isinstance(commit_dict, dict):
        author_dict = commit_dict.get("author", {})
        if isinstance(author_dict, dict):
            ts = author_dict.get("date", "")
            if ts:
                return ts
    return datetime.now(tz=timezone.utc).isoformat()


def analyze_commits(
    ctx: RepoContext,
    *,
    include_images: bool = True,
) -> tuple[int, int, int]:
    """åŒæ—¶åˆ†æä»£ç è¡Œæ•°å’Œå›¾ç‰‡è´¡çŒ®

    æ•°æ®è·å–ç­–ç•¥ï¼š
    - Fork ä»“åº“ï¼šç›´æ¥å…‹éš†ä¸Šæ¸¸ä»“åº“ï¼Œä» origin è·å– Git logï¼ˆå®Œæ•´å†å²ï¼‰
    - é Fork ä»“åº“ï¼šå…‹éš†è‡ªå·±çš„ä»“åº“ï¼Œä» origin è·å– Git logï¼ˆå®Œæ•´å†å²ï¼‰
    - API ä»…åœ¨ git log å¤±è´¥æ—¶å…œåº•ï¼ˆæœ‰åˆ†é¡µé™åˆ¶ï¼‰

    è¿”å›: (additions, deletions, total_images)
    """
    print_color("    ğŸ“Š å¼€å§‹åˆ†æcommits...", Colors.YELLOW)

    # åŠ è½½ç¼“å­˜
    cache_data = load_cache(ctx.repo_name)

    # è·å–é»˜è®¤åˆ†æ”¯
    default_branch = "main"
    if ctx.repo_path:
        git_cmd = 'git symbolic-ref refs/remotes/origin/HEAD | sed "s@^refs/remotes/origin/@@"'
        output, returncode = run_command(git_cmd, cwd=ctx.repo_path)
        default_branch = output.strip() if returncode == 0 else "main"
        print_color(f"    â„¹ï¸  é»˜è®¤åˆ†æ”¯: {default_branch}", Colors.NC)

    # è·å– commitsï¼ˆGit log ä¼˜å…ˆï¼ŒAPI å…œåº•ï¼‰
    all_commits, is_api_fallback = _fetch_commits_with_fallback(ctx, default_branch)

    if not all_commits:
        print_color("    â„¹ï¸  æœªæ‰¾åˆ°commits", Colors.NC)
        return 0, 0, 0

    total_commits = len(all_commits)
    print_color(f"    ğŸ“Š æœ€ç»ˆä½¿ç”¨ {total_commits} ä¸ªcommits", Colors.NC)

    # æ¸…ç†è¿‡æœŸç¼“å­˜
    cache_data = clean_stale_cache(
        cache_data,
        all_commits,
        ctx.repo_name,
        is_api_fallback=is_api_fallback,
    )

    # å¤„ç†æ‰€æœ‰ commits
    total_additions, total_deletions, total_images, cache_hits, cache_misses = (
        _process_all_commits(
            all_commits=all_commits,
            cache_data=cache_data,
            ctx=ctx,
            include_images=include_images,
        )
    )

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    _print_cache_stats(cache_hits, cache_misses, total_commits)
    print_stats_summary(
        total_additions,
        total_deletions,
        total_images,
        include_images=include_images,
        prefix="    ",
    )

    # ä¿å­˜ç¼“å­˜
    save_cache(ctx.repo_name, cache_data)

    return total_additions, total_deletions, total_images


def _process_all_commits(
    *,
    all_commits: list[CommitData],
    cache_data: CacheData,
    ctx: RepoContext,
    include_images: bool,
) -> tuple[int, int, int, int, int]:
    """å¤„ç†æ‰€æœ‰ commits å¹¶ç»Ÿè®¡

    è¿”å›: (total_additions, total_deletions, total_images, cache_hits, cache_misses)
    """
    total_additions = 0
    total_deletions = 0
    total_images = 0
    cache_hits = 0
    cache_misses = 0
    processed = 0
    total_commits = len(all_commits)

    for commit in all_commits:
        sha = commit.get("sha")
        if not sha or not isinstance(sha, str):
            continue

        processed += 1
        if processed % PROGRESS_INTERVAL == 0:
            pct = processed * 100 // total_commits
            print_color(
                f"    ğŸ“Š å¤„ç†ä¸­: {processed}/{total_commits} ({pct}%)", Colors.NC
            )

        commit_url = f"https://github.com/{ctx.owner}/{ctx.repo_name}/commit/{sha}"
        cached_data = _find_cached_commit(cache_data, ctx.repo_name, commit_url)

        if cached_data:
            # ä½¿ç”¨ç¼“å­˜æ•°æ®
            additions = cached_data.get("additions", 0)
            deletions = cached_data.get("deletions", 0)
            images = cached_data.get("images", 0)
            if isinstance(additions, int):
                total_additions += additions
            if isinstance(deletions, int):
                total_deletions += deletions
            if isinstance(images, int):
                total_images += images
            cache_hits += 1
            continue

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè·å–è¯¦æƒ…
        cache_misses += 1
        if ctx.repo_path:
            commit_data = _get_commit_details_from_git(ctx.repo_path, sha)
        else:
            commit_data = _get_commit_details_from_api(ctx.owner, ctx.repo_name, sha)

        # è®¡ç®—ç»Ÿè®¡
        additions, deletions, images = _calculate_commit_stats(
            commit_data, include_images=include_images
        )
        total_additions += additions
        total_deletions += deletions
        total_images += images

        # æ›´æ–°ç¼“å­˜
        if ctx.repo_name not in cache_data:
            cache_data[ctx.repo_name] = []
        cache_data[ctx.repo_name].append(
            {
                "index": processed,
                "url": commit_url,
                "additions": additions,
                "deletions": deletions,
                "images": images,
                "timestamp": _get_commit_timestamp(commit),
            }
        )

    return total_additions, total_deletions, total_images, cache_hits, cache_misses


def _print_cache_stats(cache_hits: int, cache_misses: int, total_commits: int) -> None:
    """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    print_color("    ğŸ’¾ ç¼“å­˜ç»Ÿè®¡:", Colors.YELLOW)
    print_color(f"       - ç¼“å­˜å‘½ä¸­: {cache_hits} ä¸ªcommit", Colors.NC)
    print_color(f"       - ç¼“å­˜æœªå‘½ä¸­: {cache_misses} ä¸ªcommit", Colors.NC)
    if total_commits > 0:
        cache_hit_rate = cache_hits / total_commits * 100
        print_color(f"       - ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%", Colors.NC)


# ============================================================================
# ä»“åº“å¤„ç†
# ============================================================================


def process_repos(repos: list[RepoInfo], *, include_images: bool = True) -> StatsData:
    """å¤„ç†æ‰€æœ‰ä»“åº“

    Args:
        repos: ä»“åº“åˆ—è¡¨
        include_images: æ˜¯å¦ç»Ÿè®¡å›¾ç‰‡è´¡çŒ®
    """
    print_separator("å¼€å§‹å¤„ç†ä»“åº“...")

    total_additions = 0
    total_deletions = 0
    total_images = 0
    temp_dir = Path.cwd() / "temp_repos"
    temp_dir.mkdir(parents=True, exist_ok=True)

    try:
        for repo in repos:
            repo_name = repo.get("name")
            repo_url = repo.get("html_url")
            is_fork = repo.get("fork", False)

            if not repo_name or not repo_url:
                continue

            print_separator(f"ğŸ“¦ ä»“åº“: {repo_name}", Colors.YELLOW)
            print_color("  URL: " + str(repo_url), Colors.NC)
            print_color(
                "  ç±»å‹: " + ("Fork ä»“åº“" if is_fork else "åŸåˆ›ä»“åº“"), Colors.NC
            )

            # ç¡®å®šè¦å…‹éš†çš„ä»“åº“ï¼ˆFork ä»“åº“ç›´æ¥å…‹éš†ä¸Šæ¸¸ï¼‰
            upstream_owner, upstream_name = get_upstream_repo(repo)
            if upstream_owner and upstream_name:
                # Fork ä»“åº“ï¼šç›´æ¥å…‹éš†ä¸Šæ¸¸ä»“åº“
                clone_owner = upstream_owner
                clone_repo_name = upstream_name
                target_repo_name = upstream_name
                clone_url = (
                    f"https://{TOKEN}@github.com/{upstream_owner}/{upstream_name}.git"
                )
                print_color(
                    f"  ğŸ“¡ ç›´æ¥å…‹éš†ä¸Šæ¸¸ä»“åº“: {upstream_owner}/{upstream_name}",
                    Colors.NC,
                )
            else:
                # é Fork ä»“åº“ï¼šå…‹éš†è‡ªå·±çš„ä»“åº“
                clone_owner = ORIGIN_USERNAME
                clone_repo_name = str(repo_name)
                target_repo_name = str(repo_name)
                clone_url = str(repo_url).replace(
                    "https://github.com/",
                    f"https://{TOKEN}@github.com/",
                )

            # å…‹éš†ä»“åº“åˆ°ä¸´æ—¶ç›®å½•ï¼ˆä½¿ç”¨ä¸Šæ¸¸ä»“åº“åä½œä¸ºç›®å½•åï¼‰
            repo_path = temp_dir / clone_repo_name
            if repo_path.exists():
                print_color("  ğŸ”„ æ›´æ–°æœ¬åœ°ä»“åº“...", Colors.YELLOW)
                # å…ˆå°è¯• unshallowï¼ˆå¦‚æœä¹‹å‰æ˜¯æµ…å…‹éš†ï¼‰ï¼Œç„¶å fetch
                run_command(
                    "git fetch --unshallow origin 2>/dev/null || git fetch origin",
                    cwd=str(repo_path),
                )
            else:
                print_color("  ğŸ“¥ å…‹éš†ä»“åº“...", Colors.YELLOW)
                # ä½¿ç”¨ --no-single-branch ç¡®ä¿è·å–æ‰€æœ‰åˆ†æ”¯å¼•ç”¨
                _, returncode = run_command(
                    f"git clone --no-single-branch {clone_url}",
                    cwd=str(temp_dir),
                )
                if returncode != 0 or not repo_path.exists():
                    print_color("  âš ï¸  å…‹éš†ä»“åº“å¤±è´¥ï¼Œè·³è¿‡", Colors.YELLOW)
                    continue

            # æ„å»ºä»“åº“ä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ï¼šä¸å†éœ€è¦ upstream ä¿¡æ¯ï¼Œå› ä¸ºç›´æ¥å…‹éš†çš„å°±æ˜¯ä¸»ä»“åº“ï¼‰
            ctx = RepoContext(
                repo_path=str(repo_path),
                owner=clone_owner,
                repo_name=target_repo_name,
                username=ORIGIN_USERNAME,
            )

            # åŒæ—¶åˆ†æä»£ç è¡Œæ•°å’Œå›¾ç‰‡è´¡çŒ®
            repo_additions, repo_deletions, repo_images = analyze_commits(
                ctx, include_images=include_images
            )

            total_images += repo_images

            # æ˜¾ç¤ºç»“æœ
            if repo_additions == 0 and repo_deletions == 0 and repo_images == 0:
                print_color("  âš ï¸  ç”¨æˆ·æ²¡æœ‰ä»£ç æˆ–å›¾ç‰‡è´¡çŒ®", Colors.YELLOW)
            else:
                print_stats_summary(
                    repo_additions,
                    repo_deletions,
                    repo_images,
                    include_images=include_images,
                    prefix="  ",
                )

                # ç´¯åŠ åˆ°æ€»è®¡
                total_additions += repo_additions
                total_deletions += repo_deletions
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆç¡®ä¿å¼‚å¸¸æ—¶ä¹Ÿèƒ½æ¸…ç†ï¼‰
        print_color("\n  ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...", Colors.YELLOW)
        if temp_dir.exists():
            shutil.rmtree(temp_dir)

    print_separator("ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
    print_color(f"  â• æ€» additions: {total_additions}", Colors.GREEN)
    print_color(f"  â– æ€» deletions: {total_deletions}", Colors.GREEN)
    if include_images:
        print_color(f"  ğŸ–¼ï¸ æ€» images: {total_images} images", Colors.GREEN)
    print_separator()

    return {
        "total_additions": total_additions,
        "total_deletions": total_deletions,
        "total_images": total_images,
    }


# ============================================================================
# README æ›´æ–°
# ============================================================================


def update_usernames_in_readme(content: str) -> str:
    """æ™ºèƒ½æ›´æ–° README ä¸­çš„ç”¨æˆ·åï¼ˆæ”¯æŒåŒå‘æ›¿æ¢ï¼‰

    ç­–ç•¥ï¼š
    - æ›´æ–° README é¡¶éƒ¨çš„å˜é‡å®šä¹‰ï¼šORIGIN_USERNAME = å’Œ UPSTREAM_USERNAME =
    - æ™ºèƒ½è¯†åˆ«å½“å‰çŠ¶æ€ï¼šå¦‚æœæ˜¯å ä½ç¬¦å°±æ›¿æ¢ä¸ºçœŸå®ç”¨æˆ·åï¼Œå¦‚æœæ˜¯çœŸå®ç”¨æˆ·åå°±ä¿æŒä¸å˜
    - æ”¯æŒå¯é‡å¤è¿è¡Œï¼šæ¯æ¬¡è¿è¡Œéƒ½èƒ½æ­£ç¡®å¤„ç†
    """
    # æ›´æ–°å˜é‡å®šä¹‰
    content = update_variable_definition(content, "ORIGIN_USERNAME", ORIGIN_USERNAME)
    content = update_variable_definition(
        content,
        "UPSTREAM_USERNAME",
        UPSTREAM_USERNAME,
    )

    # æ™ºèƒ½æ›¿æ¢å ä½ç¬¦
    placeholder_count = content.count("{{ORIGIN_USERNAME}}") + content.count(
        "{{UPSTREAM_USERNAME}}",
    )

    if placeholder_count > 0:
        # å‘ç°å ä½ç¬¦ï¼Œè¿›è¡Œæ›¿æ¢
        replacements = {
            "ORIGIN_USERNAME": ORIGIN_USERNAME,
            "UPSTREAM_USERNAME": UPSTREAM_USERNAME,
        }
        content = replace_placeholders(content, replacements)
        print_color(f"âœ… å·²æ›¿æ¢ {placeholder_count} ä¸ªå ä½ç¬¦ä¸ºçœŸå®ç”¨æˆ·å", Colors.GREEN)
    else:
        # æ²¡æœ‰å ä½ç¬¦ï¼Œè¯´æ˜å·²ç»æ˜¯çœŸå®ç”¨æˆ·åäº†
        print_color("â„¹ï¸  æœªå‘ç°å ä½ç¬¦ï¼Œå†…å®¹å·²åŒ…å«çœŸå®ç”¨æˆ·å", Colors.YELLOW)

    return content


def generate_readme_from_template(template_path: Path, stats: StatsData) -> str:
    """ä»æ¨¡æ¿ç”Ÿæˆ README å†…å®¹"""
    with template_path.open(encoding="utf-8") as f:
        content = f.read()

    # å‡†å¤‡æ›¿æ¢æ•°æ®
    current_time = get_current_time()
    replacements = {
        "ORIGIN_USERNAME": ORIGIN_USERNAME,
        "UPSTREAM_USERNAME": UPSTREAM_USERNAME,
        "TOTAL_ADDITIONS": str(stats.get("total_additions", 0)),
        "TOTAL_DELETIONS": str(stats.get("total_deletions", 0)),
        "TOTAL_IMAGES": str(stats.get("total_images", 0)),
        "LAST_UPDATED": current_time,
    }

    # æ›¿æ¢æ‰€æœ‰å ä½ç¬¦
    content = replace_placeholders(content, replacements)
    print_color("âœ… å·²ä»æ¨¡æ¿ç”Ÿæˆå®Œæ•´çš„ README", Colors.GREEN)

    return content


def update_existing_readme(
    content: str,
    stats: StatsData,
) -> str:
    """æ›´æ–°ç°æœ‰ README å†…å®¹"""
    # æ›¿æ¢ç»Ÿè®¡æ•°å­—
    add = stats.get("total_additions", 0)
    dele = stats.get("total_deletions", 0)
    img = stats.get("total_images", 0)
    replacement = f"\\g<1>{add}\\g<2>{dele}\\g<3>{img}"
    content = re.sub(STATS_PATTERN, replacement, content)

    # æ›´æ–°æ—¶é—´æˆ³
    current_time = get_current_time()
    time_replacement = f"\\g<1>{current_time}"
    content = re.sub(TIME_PATTERN, time_replacement, content)

    # æ›´æ–°ç”¨æˆ·å
    return update_usernames_in_readme(content)


def save_readme_content(content: str) -> bool:
    """ä¿å­˜ README å†…å®¹åˆ°æ–‡ä»¶"""
    try:
        with README_FILE_PATH.open("w", encoding="utf-8") as f:
            f.write(content)
    except OSError as e:
        print_color(f"âŒ ä¿å­˜ README å¤±è´¥: {e}", Colors.RED)
        return False
    else:
        return True


def print_update_summary(stats: StatsData) -> None:
    """æ‰“å°æ›´æ–°ç»“æœæ‘˜è¦"""
    current_time = get_current_time()
    print_color("âœ… README.md æ›´æ–°æˆåŠŸï¼", Colors.GREEN)
    print_color(f"   â• å¢åŠ è¡Œæ•°: {stats.get('total_additions', 0)}", Colors.NC)
    print_color(f"   â– åˆ é™¤è¡Œæ•°: {stats.get('total_deletions', 0)}", Colors.NC)
    print_color(f"   ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {stats.get('total_images', 0)}", Colors.NC)
    print_color(f"   ğŸ•’ æ›´æ–°æ—¶é—´: {current_time}", Colors.NC)
    print_color(f"   ğŸ‘¤ è¿œç«¯ç”¨æˆ·å: {ORIGIN_USERNAME}", Colors.NC)
    print_color(f"   ğŸ‘‘ ä¸Šæ¸¸ç”¨æˆ·å: {UPSTREAM_USERNAME}", Colors.NC)


def _read_current_stats_from_readme() -> StatsData | None:
    """ä»ç°æœ‰ README.md ä¸­è¯»å–å½“å‰çš„ç»Ÿè®¡æ•°å­—

    è¿”å›: åŒ…å« total_additions, total_deletions, total_images çš„å­—å…¸ï¼Œæˆ– None
    """
    if not README_FILE_PATH.exists():
        return None

    try:
        with README_FILE_PATH.open(encoding="utf-8") as f:
            content = f.read()
        match = re.search(STATS_EXTRACT_PATTERN, content)
        if match:
            return {
                "total_additions": int(match.group(1)),
                "total_deletions": int(match.group(2)),
                "total_images": int(match.group(3)),
            }
    except (OSError, UnicodeDecodeError, ValueError):
        pass

    return None


def update_readme(stats: StatsData) -> bool:
    """æ›´æ–° README.md ä¸­çš„ç»Ÿè®¡æ•°æ®å’Œæ—¶é—´ï¼ˆæ”¯æŒæ¨¡æ¿ç³»ç»Ÿï¼‰

    åŠŸèƒ½ï¼š
    - å¦‚æœå­˜åœ¨ README.template.mdï¼Œä»æ¨¡æ¿ç”Ÿæˆå®Œæ•´çš„ README
    - å¦‚æœä¸å­˜åœ¨æ¨¡æ¿ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›´æ–°ç°æœ‰ README
    - å¦‚æœç»Ÿè®¡æ•°æ®æœªå˜åŒ–ï¼Œè·³è¿‡æ›´æ–°é¿å…æ— æ„ä¹‰çš„æäº¤
    - æ”¯æŒå¯é‡å¤è¿è¡Œï¼Œå®Œç¾è§£å†³å ä½ç¬¦æ›¿æ¢é—®é¢˜

    å‚æ•°ï¼š
    - stats: ç»Ÿè®¡æ•°æ®å­—å…¸ï¼ŒåŒ…å« total_additions, total_deletions, total_images
    """
    print_color("ğŸ“ æ›´æ–° README.md...", Colors.YELLOW)

    # å…ˆæ£€æŸ¥ç»Ÿè®¡æ•°æ®æ˜¯å¦æœ‰å˜åŒ–
    old_stats = _read_current_stats_from_readme()
    if (
        old_stats is not None
        and old_stats.get("total_additions") == stats.get("total_additions", 0)
        and old_stats.get("total_deletions") == stats.get("total_deletions", 0)
        and old_stats.get("total_images") == stats.get("total_images", 0)
    ):
        print_color("â„¹ï¸  ç»Ÿè®¡æ•°æ®æœªå˜åŒ–ï¼Œè·³è¿‡ README æ›´æ–°", Colors.YELLOW)
        return True

    template_path = Path(__file__).parent.parent / "README.template.md"

    if template_path.exists():
        # ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿ
        print_color("ğŸ“„ ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿç”Ÿæˆ README", Colors.GREEN)
        content = generate_readme_from_template(template_path, stats)
    else:
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ›´æ–°ç°æœ‰ README
        print_color("âš ï¸  æœªå‘ç°æ¨¡æ¿æ–‡ä»¶ï¼Œæ›´æ–°ç°æœ‰ README", Colors.YELLOW)

        if not README_FILE_PATH.exists():
            print_color("âŒ README.md ä¸å­˜åœ¨ï¼", Colors.RED)
            return False

        # è¯»å–ç°æœ‰ README.md
        with README_FILE_PATH.open(encoding="utf-8") as f:
            existing_content = f.read()

        content = update_existing_readme(existing_content, stats)

    # ä¿å­˜ README.md
    if not save_readme_content(content):
        return False

    # æ˜¾ç¤ºæ›´æ–°ç»“æœ
    print_update_summary(stats)
    return True


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


def main() -> int:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ GitHub ç»Ÿè®¡")
    parser.add_argument("--no-images", action="store_true", help="ä¸ç»Ÿè®¡å›¾ç‰‡è´¡çŒ®")
    parser.add_argument("--clear-cache", action="store_true", help="æ¸…é™¤ç¼“å­˜æ–‡ä»¶")
    args = parser.parse_args()

    print_separator("ğŸš€ å¼€å§‹ç”Ÿæˆ GitHub ç»Ÿè®¡...")
    print_color("ğŸ“Š ç»Ÿè®¡é…ç½®:", Colors.YELLOW)
    print_color(f"   - å›¾ç‰‡ç»Ÿè®¡: {'å…³é—­' if args.no_images else 'å¼€å¯'}", Colors.NC)
    if not args.no_images:
        print_color(f"   - ç¼“å­˜ç›®å½•: {CACHE_DIR}", Colors.NC)
    print_separator()

    # å¤„ç†æ¸…é™¤ç¼“å­˜
    if args.clear_cache:
        if CACHE_DIR.exists():
            print_color(f"ğŸ—‘ï¸  æ¸…é™¤ç¼“å­˜ç›®å½•: {CACHE_DIR}", Colors.YELLOW)
            shutil.rmtree(CACHE_DIR)
            print_color("âœ… ç¼“å­˜å·²æ¸…é™¤", Colors.GREEN)
        else:
            print_color("â„¹ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: " + str(CACHE_DIR), Colors.NC)
        return 0

    # æ£€æŸ¥ TOKEN
    if not TOKEN:
        print_color("âŒ é”™è¯¯: GH_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®", Colors.RED)
        return 1

    # åŠ è½½å·²çŸ¥ä½œè€…èº«ä»½ï¼ˆåç»­å¤„ç†ä»“åº“æ—¶ä¼šå¢é‡å­¦ä¹ æ–°èº«ä»½ï¼‰
    KNOWN_AUTHOR_IDENTITIES.clear()
    KNOWN_AUTHOR_IDENTITIES.update(load_author_identities())

    # è·å–ä»“åº“åˆ—è¡¨
    repos = get_repos()
    if not repos:
        print_color("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»“åº“", Colors.YELLOW)
        return 1

    # å¤„ç†ä»“åº“
    stats = process_repos(repos, include_images=not args.no_images)

    # æ›´æ–° README.md
    update_readme(stats)

    print_separator("âœ… è„šæœ¬æ‰§è¡Œå®Œæˆï¼")
    return 0


if __name__ == "__main__":
    sys.exit(main())
